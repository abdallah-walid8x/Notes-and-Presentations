% ****** Start of file aipsamp.tex ******
%
%   This file is part of the AIP files in the AIP distribution for REVTeX 4.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 American Institute of Physics.
%
%   See the AIP README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex  aipsamp
%  2)  bibtex aipsamp
%  3)  latex  aipsamp
%  4)  latex  aipsamp
%
% Use this file as a source of example code for your aip document.
% Use the file aiptemplate.tex as a template for your document.
\documentclass[%
 aip,
 jmp,%
 amsmath,amssymb,
%preprint,%
 reprint,%
%author-year,%
%author-numerical,%
]{revtex4-2}
\usepackage{tikz}
\usetikzlibrary{patterns,plotmarks}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{float}
\raggedbottom
\usepackage{listings}
\usepackage{xcolor}

\lstset{
  basicstyle=\ttfamily\small,   % small font, monospaced
  numbers=left,                 % line numbers on the left
  numberstyle=\tiny\color{gray},% line numbers style
  keywordstyle=\color{blue},    % keywords in blue
  commentstyle=\color{green!50!black}, % comments in green
  stringstyle=\color{red!70!brown},   % strings in red
  breaklines=true,              % automatic line breaking
  frame=single,                 % frame around code
  tabsize=2
}

%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{graphicx}
\usepackage{float}

\begin{document}

\preprint{AIP/123-QED}

\title[CERN ROOT and Pythia8: Analysis and Study]{CERN ROOT and Pythia8: Analysis and Study}% Force line breaks with \\


\author{Abdallah W. Mahmoud}
 %\altaffiliation{The American University in Cairo, Department of Physics}%Lines break automatically or can be forced with \\
%\author{B. Author}%
 \email{abdallah\_walid8@aucegypt.edu}
\affiliation{ 
The American University in Cairo, Department of Physics%\\This line break forced with \textbackslash\textbackslash
}%




\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
This paper provides a comprehensive overview of ROOT and Pythia, and shows several projects which demonstrate the applications of both tools in particle physics for analysis and study of physical phenomena. No real data was used, and all the data used to plot the graphs was randomly generated in the codes. The codes for all projects are shown in the appendix with comments. 
%
\end{abstract}

\keywords{ROOT, Pythia, STAR, event, resonance, spectrum, fit, dataset, RHIC, LHC, statistics, HEP, proton-proton collisions, QCD, QGP}%Use showkeys class option if keyword
                              %display desired
\maketitle

\begin{quotation}
CERN ROOT and Pythia8 are two of the most widely used tools in high-energy particle physics. Modern HEP experiments rely on sophisticated software frameworks to simulate, analyze, and interpret immense volumes of data generated by particle collisions. This is where ROOT and Pythia8 are used; they form a powerful and flexible computational ecosystem that enables researchers to model fundamental interactions and extract meaningful physics results from raw detector data.
\end{quotation}

\section{\label{sec:level1}Introduction to CERN ROOT}

Widely considered to be the backbone of HEP data analysis, ROOT is a comprehensive data analysis framework that provides Efficient I/O capabilities, advanced statistical tools, and machine learning functionalities. Adopted extensively by CERN and STAR, it allows physicists to structure, explore, and quantify the results of simulated or recorded events, bridging the gap between theoretical models and experimental observables. ROOT's applications span data acquisition, simulation, reconstruction, and statistical analysis in HEP. It excels in handling complex datasets from particle detectors, enabling tasks such as event reconstruction, Monte Carlo simulations, and multidimensional data fitting. The following projects demonstrate the different ways ROOT can be used.

\subsection{\label{sec:level2}Importing randomly generated data into a simple histogram}

This project generates a 1D Gaussian data set ($\mu$ = 5, $\sigma$ = 1) to mimic any measured quantity affected by random noise. The values are stored in a file, read and filled into a histogram, which is then fitted with the built-in Gaussian function of ROOT. This exercise introduces the basic concepts of histogramming, fitting, and parameter extraction in experimental analysis.\newline

The Gaussian fit returns three key parameters: amplitude (peak height), mean, and standard deviation. The mean represents the central value of the observable (e.g., a calibration point or resonance mass). The standard deviation quantifies the resolution or spread of the measurement. The amplitude, combined with the standard deviation, can be integrated to estimate the total number of events. The ratio $\mu/\sigma$, though printed in the code, does not represent a standard statistical metric but illustrates how the derived quantities can be formed from the results of the fit. The code also adds legends, arrows, and annotations to highlight features in the plot. Although pedagogical, this mirrors how experimental plots are annotated to draw attention to key structures. It emphasizes that clear visualization is as important as numerical results, especially when communicating findings.

\begin{center}
   
    \includegraphics[width=1\linewidth]{extractandplot run.png} % include the compiled PDF
     
\end{center}

N = 1000 measurements were simulated and then drawn from a Gaussian distribution with true mean $\mu$ = 5.0 and detector resolution $\sigma$ = 1.0 to illustrate the analysis workflow. The events were histogrammed into one hundred bins in the range 0–15, and a binned fit was performed using a Gaussian PDF (ROOT built-in gaus). The Gaussian parameters returned by the fit are reported with $\pm1\sigma$ statistical uncertainties obtained from the fit covariance matrix.

\begin{center}
   
    \includegraphics[width=1\linewidth]{ROOT_and_Pythia (1).pdf} % include the compiled PDF
     
\end{center}
The fitted Gaussian parameters are $\mu$ = 5.02 ± 0.03 and $\sigma$ = 0.97 ± 0.02 (statistical). The fit reproduces the injected parameters within statistical uncertainty. Conceptually, this project demonstrates how Gaussian statistics underlie measurement analysis. Random fluctuations naturally follow normal distributions due to the Central Limit Theorem, and Gaussian fits provide precise estimates of central values and uncertainties. This makes the example relevant to countless physical measurements: detector resolution studies, energy calibration, or mass peak reconstruction.

\subsection{\label{sec:level2}Analyzing the $J/\psi$ resonance}

This code simulates and analyzes the invariant mass spectrum of muon pairs ($\mu^+ \mu^-$) in the energy range 2–4 GeV. It models a $J/\psi$ resonance using a Gaussian distribution superimposed on an exponential background. The Gaussian approximates detector-smeared resonance signals, while the exponential describes combinatorial background.\newline

This setup reflects how experimental particle physicists identify and quantify resonance peaks within noisy data. The central task is a fit with a composite function: exponential added to Gaussian. The Gaussian’s mean corresponds to the measured resonance mass, its width $\sigma$ to the detector resolution, and its amplitude to the signal strength. The exponential’s slope and normalization parameterize the falling background. From these parameters, one can compute physically meaningful observables: signal yield (area under Gaussian), background estimate under the peak, and statistical significance of the signal.
\begin{center} 
\includegraphics[width=1\linewidth]{extract2 run.png} % include the compiled PDF     
\end{center}

A fit is performed to the invariant mass spectrum of reconstructed muon pairs in the range 2.0–4.0 GeV/$c^2$ using a model consisting of a Gaussian signal for the $J/\psi$ resonance plus an exponential background. The Gaussian center and width measure the reconstructed $J/\psi$ mass and the detector mass resolution respectively. The total signal yield is obtained by integrating the Gaussian component, and the background contribution in the signal window is estimated by integrating the exponential component in the same region. Systematic uncertainties were evaluated by varying the background parametrization (exponential $\leftrightarrow$ polynomial) and fit range; final yields include these variations as an uncertainty.\newline

This workflow illustrates a general paradigm in particle physics: distinguishing signal from background using fitting techniques. It emphasizes the importance of understanding model choice, parameter interpretation and uncertainties. For instance, the $J/\psi$ natural width is negligible compared to detector resolution, justifying the Gaussian approximation. Other factors, such as the background model, fit range and binning, must also be considered when analyzing real data.\newline

\begin{center}
\includegraphics[width=1.15\linewidth]{ROOT_and_Pythia.pdf} % include the compiled PDF    
\end{center}


Overall, this project highlights how statistical modeling transforms raw histograms into physics results. It is the norm of resonance searches: build a physically motivated fit function, extract signal yields, and evaluate significance. The methodology applies not only to quarkonium studies but also to any resonance search, such as Higgs or Z$^\prime$  bosons.



\subsection{\label{sec:level2}Importing and Analyzing Data from multiple files}

This project demonstrates a robust workflow for handling multiple data files, a common scenario in HEP where data from different runs or detectors must be merged. It generates 10 synthetic input files, each containing 1000 events across six datasets, with each dataset drawn from a Gaussian distribution of varying mean ($\mu$ = -1 to 1 GeV/c) and width ($\sigma$ = 0.6 to 2.0 GeV/c). The code reads these files back, filling ROOT histograms (TH1F), which mirrors processing large-scale experimental data (from ATLAS or CMS for example). This concept of modular file handling ensures scalability and robustness against issues like file corruption, a key consideration in real analyses. A central concept is fitting Gaussian distributions to extract parameters (mean, sigma) using ROOT's TF1 and Fit functions, which perform $\chi^2$ minimization. Each histogram is fitted independently, recovering input parameters with small errors (~0.01 GeV/c due to 10,000 entries), and the $\chi^{2}/\mathrm{ndf}$ being very close to 1 confirms good fits. The use of THStack to overlay histograms with distinct colors and markers, combined with overlaid fit curves and a legend, shows effective data visualization. This reflects HEP practices for modeling detector resolutions, particle masses, or backgrounds, where Gaussians approximate smearing effects. Printing fit parameters (with errors) and entries facilitates quantitative comparisons, useful for tables in papers summarizing resolution studies or systematic variations across datasets.


\begin{center}
   
    \includegraphics[width=1\linewidth]{multiplefilesupgrade run.png} % include the compiled PDF
     
\end{center}

The plot below  displays six Gaussians, highlighting differences in peak position and width, which could represent different particle types or detector conditions. This concept of comparative visualization is critical for presenting multi-component analyses in HEP, such as comparing particle spectra across runs or detectors, and inspires clear, informative figures in research papers.

\begin{center}

    \includegraphics[width=1\linewidth]{momentum_distributions_with_fits.pdf} % include the compiled PDF
    
\end{center}

This code is ideal for inspiring a paper section on data processing and statistical analysis. The multi-file approach could be adapted to real data, and the fitting routine could model detector effects or particle signals. The overlay plot is perfect for illustrating variations, such as momentum resolutions, and the fit table could summarize results.





\subsection{\label{sec:level2}Analyzing the $\rho$ meson resonance}

This project simulates an invariant mass distribution for 100,000 events, with 10 percent signal (Breit--Wigner fitted, {mimicking }$\rho$ mesons) and 90 percent background (Gaussian fitted). This reflects HEP data storage from experiments related to particle decays. The concept of mixing signals received by the detector (identified by resonance peaks in the data) and background noise (identified by combinatorial or resolution effects) is central to resonance searches, such as identifying vector mesons in RHIC/STAR data. In other words, the choice of distributions (Gaussian + exponential vs. Breit-Wigner + Gaussian) and mass ranges reflect different physical scenarios and analysis goals.



\begin{center}
\includegraphics[width=1\linewidth]{macro3 run.png} % include the compiled PDF     
\end{center}
\begin{center}

    \includegraphics[width=1\linewidth]{c.pdf} % include the compiled PDF
    
\end{center}

The data is read into a histogram and fitted separately with Gaussian (representing background) and Breit-Wigner (representing the signal) functions, demonstrating basic signal extraction. The fits, initialized near input parameters, yield high $\chi^{2}/\mathrm{ndf}$ due to separate fitting of a mixed distribution, highlighting the need for combined models in real analyses. The concept of fitting to extract resonance parameters such as mass, width, and yield is critical in HEP for quantifying particle production and testing physics models (such as QGP effects on $\rho$ mesons). The plot overlays data points with error bars, the Gaussian fit as a blue line, and the Breit-Wigner fit as a red line, with a legend clarifying components. The histogram shows a broad background peak with a smaller signal peak. This visualization is key for HEP papers, illustrating signal significance and fit challenges.\newline

This code can inspire a paper section on signal extraction, with a figure showing the mass spectrum and a table of fit parameters. By implementing a combined signal$+$background fit (e.g., gaus$+$breitwigner), it is possible to extract yield and significance, or compare to theoretical predictions (e.g., $\rho$ production in pp vs. Au-Au collisions). This workflow, summarized by the terms: generate, store, analyze, visualize—is a strong template for resonance studies.

\subsection{\label{sec:level2}Creating a 2D histogram}

This project simulates a two-dimensional Gaussian distribution in x and y with zero mean and unitary standard deviation. This can represent, for instance, the spatial distribution of a particle beam spot or detector response. A million events are generated and filled into a 2D histogram, with projections shown along each axis. This is a common method in collider physics to study beam alignment and detector resolution.\newline

The 2D histogram reveals the overall shape of the distribution: a circular Gaussian cloud when x and y are independent. If the distribution were elliptical, it would indicate different resolutions along the axes or correlations between x and y. By computing covariance and correlation coefficients, one quantitatively checks for coupling or misalignment between the axes. In this simulation, the correlation is close to zero, confirming independence.\newline

Projections onto x and y are fitted with Gaussian functions to extract precise estimates of mean and resolution for each coordinate. These fits provide uncertainties smaller than the raw RMS because they model the statistical distribution explicitly. This approach mirrors experimental procedures, where marginal distributions are fitted to quantify beam spot or vertex resolutions. The project also introduces advanced statistical ideas. The covariance matrix encodes both variances and correlations, and its diagonalization reveals principal axes (major/minor widths and orientation). This is the foundation of describing elliptical Gaussian distributions, widely used in accelerator physics and detector alignment. Thus, the code exemplifies both practical visualization (heatmaps, projections) and the theoretical framework for understanding multidimensional Gaussian statistics.

\begin{center}
   
    \includegraphics[width=1\linewidth]{two_d_histogram run.png} % include the compiled PDF
     
\end{center}

\begin{center}

    \includegraphics[width=1\linewidth]{beamspot_2D_with_projections.pdf} % include the compiled PDF
    
\end{center}

 The two-dimensional spatial distribution of reconstructed vertices were simulated and analyzed to characterize the beam spot. A dataset of 1,000,000 events was generated with independent Gaussian distributions in x and y ($\mu$=0, $\sigma$=1 cm). The event density was binned into a 200×200 two-dimensional histogram over $-3$ to $3$ cm and plotted as a heatmap with contour overlays. One-dimensional projections onto x and y were fitted with Gaussian functions to extract mean positions and resolutions. The covariance matrix was computed from the sample to quantify correlations between axes, and principal axes were obtained by diagonalizing the covariance matrix. Statistical uncertainties on fitted parameters were obtained from the fit covariance matrices.

\subsection{\label{sec:level2}Creating a 3D visual}

This project uses ROOT's TGeoManager to create a simplified 3D model of the STAR detector, including a TPC barrel (silicon tube), endcaps (aluminum disks), and an inner tracker (silicon layer). Defining materials (vacuum, aluminum, silicon) with physical properties (A, Z, density) and constructing hierarchical volumes (TGeoVolume) is a key concept, mirroring GEANT-based simulations for particle tracking. This geometry setup is essential for understanding detector interactions such as energy loss and scattering, and the simplified model here illustrates core components for visualization, critical for papers or presentations.


\begin{center}
   
    \includegraphics[width=1\linewidth]{view2_3d run.png} % include the compiled PDF
     
\end{center}

The code generates five helical tracks to simulate charged particle trajectories in STAR’s solenoidal magnetic field along the z-axis. Tracks spiral in the x-y plane (radius $\alpha p_T / B$) and advance linearly in z, with random starting angles and radii to mimic varying momenta. This concept is fundamental in HEP for tracking studies, where curvature yields momentum and vertex positions aid reconstruction, such as in heavy-flavor decays. Visualizing tracks within the geometry tests detector coverage, inspiring event-display figures in research papers. Using OpenGL (Draw("ogl")) for interactive 3D rendering, with outline style and camera rotation, provides a clear view of detector components and tracks. Transparency and color settings enhance visibility. This visualization technique is vital for illustrating detector design or simulation validation, often used in technical notes or conference talks to convey experimental setup.

\begin{center}

    \includegraphics[width=1\linewidth]{visual2.png} % include the compiled PDF
    
\end{center}


This project can inspire a "Detector Simulation" section with a 3D figure showing STAR’s layout or track reconstruction. It could be extended with real GEANT geometry or integrated with event generators to show realistic events. The figure is ideal for discussing acceptance or tracking efficiency, and the code’s modularity allows adding more detector layers (such as calorimeters) for a more complete model, enhancing its academic value.


\section{\label{sec:level1}Introduction to Pythia8}

Pythia8 is mainly associated with event generation from first principles. Basically, it is a Monte Carlo event generator that simulates high-energy collisions between fundamental particles, such as those occurring at the Large Hadron Collider or the Relativistic Heavy Ion Collider. Pythia8 enables theorists and experimentalists to generate "truth-level" events, which serve as reference for detector simulations and the main validation tool used to validate or refute physics models. Its modular C++ structure, customizability, and compatibility with detector-level frameworks make it a foundational tool in both STAR and CERN research environments. \newline



Proton-proton (pp) collisions occur when two protons are accelerated to high energies and smashed together in particle accelerators like the Large Hadron Collider at CERN or the Relativistic Heavy Ion Collider at Brookhaven. At everyday energies, protons behave as stable composite particles made of three valence quarks (two up and one down) bound by gluons, the carriers of the strong nuclear force described by Quantum Chromodynamics. However, in high-energy collisions (such as $\sqrt{s}$ = 13 TeV at LHC or 200 GeV at RHIC), the protons' internal structure becomes relevant: they are clouds of quarks, antiquarks, and gluons (collectively called partons) whose distributions are encoded in Parton Distribution Functions. Collisions can be classified as elastic, where protons scatter intact, or inelastic, where protons break apart (producing new particles), or diffractive, where one or both protons remain intact but exchange momentum via Pomeron exchange, a QCD phenomenon. Inelastic collisions dominate at high energies, leading to a cascade of processes from initial parton interactions to final hadron production.\newline

The total cross-section for pp collisions at LHC energies is around 100 millibarns, with QCD processes contributing the majority due to the strong force's dominance over electroweak interactions. Key observables include particle multiplicity (number of produced particles), transverse momentum ($p_T$) spectra, and correlations, which probe the proton's structure and QCD dynamics. These collisions serve as a baseline for studying heavy-ion physics, such as the quark-gluon plasma, and searching for new physics, but QCD effects like multiple parton interactions can create backgrounds that mimic rare signals. In contrast to soft QCD, hard processes involve high $p_T$ ($>$ few GeV/c) where $\alpha_s$ is small ($~$0.1$-$0.3), allowing pQCD calculations. These include jet production from 2→2 parton scatters (e.g., gg$\rightarrow$gg, qg$\rightarrow$qg). Parton showers add perturbative corrections, modeled as branching processes. Hard QCD probes $\alpha_s$ evolution and gluon PDFs, dominant at low x, where x is parton momentum fraction. At LHC, hard processes like inclusive jets or multijets are measured differentially, constraining PDFs and extracting $\alpha_s$ at Z-boson mass scale with NNLO accuracy. Factorization separates hard matrix elements from PDFs and showers, enabling predictions. Hard QCD constitutes backgrounds for new physics but also tests QCD at high scales, with some correlations being sensitive to new colored particles. Beyond QCD, electroweak processes occur, though with smaller cross-sections. These include Drell-Yan processes, W/Z production, and Higgs boson decays. QCD contributes to these via higher-order corrections (such as gluon radiation) and backgrounds (such as multijet fakes).\newline

In pp collisions, new physics searches (e.g., supersymmetry, extra dimensions) rely on QCD modeling for backgrounds. Collective phenomena in high-multiplicity pp events suggest QGP-like effects, blurring lines with heavy-ion collisions, possibly from hydrodynamic flow or string shoving. Recent advances in QCD theory, such as reconciling quantum and relativistic aspects, improve proton structure understanding, linking collisions to internal dynamics via better data interpretation. Overall, pp collisions are a fundamental in QCD, with soft processes dominating bulk production and hard ones probing high scales, essential for precision physics at future colliders.\newpage

\subsection{\label{sec:level2}Proton-Proton collisions}

This project on proton-proton collisions code uses Pythia8, a Monte Carlo event generator, to simulate proton-proton (pp) collisions at 200 GeV, mimicking conditions at the Relativistic Heavy Ion Collider (RHIC) for the STAR experiment. A critical concept is the simulation of hard QCD processes (enabled via HardQCD:all = on), which involve high transverse momentum ($p_T$) parton scatterings (e.g., quark-gluon interactions) described by perturbative QCD (pQCD). Pythia8 models the full event evolution, including initial-state radiation, parton showers, multiple parton interactions, and hadronization via the Lund string model, transforming partons into observable hadrons. This allows researchers to predict particle spectra and test theoretical models against experimental data, a cornerstone of HEP simulation workflows.


\begin{center}

    \includegraphics[width=1\linewidth]{pp 1.png} % include the compiled PDF
    
\end{center}

\begin{center}

    \includegraphics[width=1\linewidth]{pp 2.png} % include the compiled PDF
    
\end{center}

\begin{center}

    \includegraphics[width=1\linewidth]{pp 3.png} % include the compiled PDF
    
\end{center}

\begin{center}

    \includegraphics[width=1\linewidth]{pp 4.png} % include the compiled PDF
    
\end{center}

\begin{center}

    \includegraphics[width=1\linewidth]{pp 5.png} % include the compiled PDF
    
\end{center}

\begin{center}

    \includegraphics[width=1\linewidth]{pp 6.png} % include the compiled PDF
    
\end{center}


\begin{center}

    \includegraphics[width=1\linewidth]{pp 7.png} % include the compiled PDF
    
\end{center}


\begin{center}
   
    \includegraphics[width=1\linewidth]{pT_distribution.pdf} % include the compiled PDF
     
\end{center}


\begin{center}
   
    \includegraphics[width=1\linewidth]{eta_distribution.pdf} % include the compiled PDF
     
\end{center}




\begin{center}
   
    \includegraphics[width=1\linewidth]{phi_distribution.pdf} % include the compiled PDF
     
\end{center}
The code generates histograms for key kinematic variables—$p_T$ (transverse momentum), $\eta$ (pseudorapidity), and $\phi$ (azimuthal angle)—using ROOT's histogram classes (TH1F, TH2F). A crucial concept is the application of detector acceptance cuts ($|\eta|$ $<$ 1, $p_T$ $>$ 0.2 GeV/c), reflecting STAR's central tracking capabilities. These cuts filter particles to those detectable by the experiment, enabling realistic comparisons with data. The histograms capture distributions: $p_T$ follows a steep exponential (soft) to power-law (hard) shape, $\eta$ is flat in the central rapidity plateau, and $\phi$ is uniform due to cylindrical symmetry. The 2D $\eta$ vs $p_T$ histogram below shows full kinematic coverage, highlighting detector limitations, which is vital for understanding experimental biases in papers.\newline
\begin{center}
   
    \includegraphics[width=1\linewidth]{pT_vs_eta.pdf} % include the compiled PDF
     
\end{center}

ROOT's visualization tools can produce publication-quality plots with logarithmic axes for $p_T$ (to emphasize tails) and a color map for 2D plots. The use of Sumw2() ensures proper statistical error propagation (Poisson uncertainties), critical for HEP analyses where precision matters. The code's output, including Pythia statistics, provides insights into process contributions (e.g., gg$\rightarrow$gg dominance). This workflow—simulate, filter, histogram, visualize—is standard for validating Monte Carlo predictions against data, inspiring sections in papers on simulation methods or kinematic results. \newline

This project exemplifies how to simulate collider events, apply experimental constraints, and analyze distributions. It could inspire a methodology section detailing Monte Carlo tools, a results section with kinematic plots, or comparisons with STAR data (e.g., $p_T$ spectra fitted with Tsallis functions). Extending the code by adding particle identification or soft QCD processes could enhance its applicability to specific physics questions, such as jet production or bulk properties in pp collisions.


\newpage





\section{\label{sec:level1}References}

\nocite{*}
\bibliography{aipsamp}% Produces the bibliography via BibTeX.



\newpage

\appendix

\section{Codes}

This appendix shows all the codes used for each project, with comments added to improve the reader's understanding of the logic and flow of the codes.

\subsection{\label{app:subsec}Importing randomly generated data into a simple histogram}

\lstinputlisting[language=C, caption={extractandplot.c}]{extractandplot.c}


\newpage

\subsection{\label{app:subsec}Analyzing the $J/\psi$ resonance}

\lstinputlisting[language=C, caption={extract2.c}]{extract2.c}

\newpage

\subsection{\label{app:subsec}Importing and Analyzing Data from multiple files}

\lstinputlisting[language=C, caption={multiplefilesupgrade.c}]{multiplefilesupgrade.c}

\newpage

\subsection{\label{app:subsec}Analyzing the $\rho$ meson resonance}

\lstinputlisting[language=C, caption={macro3.c}]{macro3.c}

\newpage

\subsection{\label{app:subsec}Creating a 2D histogram}

\lstinputlisting[language=C, caption={two_d_histogram.c}]{two_d_histogram.c}

\newpage

\subsection{\label{app:subsec}Creating a 3D visual}

\lstinputlisting[language=C, caption={view2_3d.c}]{view2_3d.c}

\newpage

\subsection{\label{app:subsec}Proton-Proton collisions}

\lstinputlisting[language=C++, caption={ppcollision.cc}]{ppcollision.cc}


%\begin{verbatim}
%\appendix
%\section{}
%\end{verbatim}
%will produce an appendix heading that says ``APPENDIX A'' and
%\begin{verbatim}
%\appendix
%\section{Background}
%\end{verbatim}
%will produce an appendix heading that says ``APPENDIX A: BACKGROUND''
%(note that the colon is set automatically).

%If there is only one appendix, then the letter ``A'' should not
%appear. This is suppressed by using the star version of the appendix
%command (\verb+\appendix*+ in the place of \verb+\appendix+).

%\section{References}

%Observe that this appendix was started by using
%\begin{verbatim}
%\section{A little more on appendixes}
%\end{verbatim}

%Note the equation number in an appendix:
%\begin{equation}
%E=mc^2.
%\end{equation}

%\subsection{\label{app:subsec}A subsection in an appendix}

%You can use a subsection or subsubsection in an appendix. Note the
%numbering: we are now in Appendix~\ref{app:subsec}.

%\subsubsection{\label{app:subsubsec}A subsubsection in an appendix}
%Note the equation numbers in this appendix, produced with the
%subequations environment:
%\begin{subequations}
%begin{eqnarray}
%E&=&mc, \label{appa}
%\\
%E&=&mc^2, \label{appb}
%\\
%E&\agt& mc^3. \label{appc}
%\end{eqnarray}
%\end{subequations}
%They turn out to be Eqs.~(\ref{appa}), (\ref{appb}), and (\ref{appc}).

%\newpage
%\bibliographystyle{unsrt}  % or plain, alpha, etc.
%\maketitle{References}
%\bibliography{references}


\end{document}
%
% ****** End of file aipsamp.tex ******
